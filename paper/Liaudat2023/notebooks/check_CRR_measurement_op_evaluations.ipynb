{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA A100-PCIE-40GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import time as time\n",
    "\n",
    "import torch\n",
    "\n",
    "M1 = False\n",
    "\n",
    "if M1:\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(torch.cuda.is_available())\n",
    "        print(torch.cuda.device_count())\n",
    "        print(torch.cuda.current_device())\n",
    "        print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import skimage as ski\n",
    "\n",
    "import large_scale_UQ as luq\n",
    "from large_scale_UQ.utils import to_numpy, to_tensor\n",
    "from convex_reg import utils as utils_cvx_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation options for the MAP estimation\n",
    "options = {\"tol\": 1e-5, \"iter\": 15000, \"update_iter\": 4999, \"record_iters\": False}\n",
    "# Save param\n",
    "repo_dir = \"./../../..\"\n",
    "base_savedir = \"/disk/xray99/tl3/proj-convex-UQ/outputs/new_UQ_results/CRR\"\n",
    "save_dir = base_savedir + \"/vars/\"\n",
    "savefig_dir = base_savedir + \"/figs/\"\n",
    "\n",
    "# Define my torch types (CRR requires torch.float32)\n",
    "myType = torch.float32\n",
    "myComplexType = torch.complex64\n",
    "\n",
    "# CRR load parameters\n",
    "sigma_training = 5\n",
    "t_model = 5\n",
    "CRR_dir_name = \"./../../../trained_models/\"\n",
    "# CRR parameters\n",
    "reg_params = [5e4]\n",
    "mu = 20\n",
    "\n",
    "\n",
    "# LCI params\n",
    "alpha_prob = 0.01\n",
    "\n",
    "# LCI algorithm parameters (bisection)\n",
    "LCI_iters = 200\n",
    "LCI_tol = 1e-4\n",
    "LCI_bottom = -10\n",
    "LCI_top = 10\n",
    "\n",
    "# Compute the MAP-based UQ plots\n",
    "superpix_MAP_sizes = [16, 8]  # [32, 16, 8, 4]\n",
    "# Clipping values for MAP-based LCI. Set as None for no clipping\n",
    "clip_high_val = 1.0\n",
    "clip_low_val = 0.0\n",
    "\n",
    "# Compute the sampling UQ plots\n",
    "superpix_sizes = [32, 16, 8, 4, 1]\n",
    "\n",
    "# Sampling alg params\n",
    "frac_delta = 0.98\n",
    "frac_burnin = 0.1\n",
    "n_samples = np.int64(5e4)\n",
    "thinning = np.int64(1e1)\n",
    "maxit = np.int64(n_samples * thinning * (1.0 + frac_burnin))\n",
    "# SKROCK params\n",
    "nStages = 10\n",
    "eta = 0.05\n",
    "dt_perc = 0.99\n",
    "\n",
    "# Plot parameters\n",
    "cmap = \"cubehelix\"\n",
    "nLags = 100\n",
    "\n",
    "\n",
    "# Img name list\n",
    "img_name_list = [\"W28\"]  # ['M31', 'W28', 'CYN', '3c288']\n",
    "# Input noise level\n",
    "input_snr = 30.0\n",
    "\n",
    "\n",
    "save_fig_vals = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- loading checkpoint from epoch 10 ---\n",
      "---------------------\n",
      "Building a CRR-NN model with \n",
      " - [1, 8, 32] channels \n",
      " - linear_spline activation functions\n",
      "  (LinearSpline(mode=conv, num_activations=32, init=zero, size=21, grid=0.010, monotonic_constraint=True.))\n",
      "---------------------\n",
      "Numbers of parameters before prunning: 13610\n",
      "---------------------\n",
      " PRUNNING \n",
      " Found 22 filters with non-vanishing potential functions\n",
      "---------------------\n",
      "Numbers of parameters after prunning: 4183\n",
      "Lipschitz bound 0.770\n",
      "[GD] 0 out of 15000 iterations, tol = 0.103799\n",
      "[GD] converged in 541 iterations\n",
      "-----------------------\n",
      "Updating spline coefficients for the reg cost\n",
      " (the gradient-step model is trained and intergration is required to compute the regularization cost)\n",
      "-----------------------\n",
      "Calculating credible interval for superpxiel:  (256, 256)\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "Calculating credible interval for superpxiel:  (256, 256)\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "[Bisection Method] There is no root in this range.\n",
      "f(x_map):  2239.935546875 \n",
      "g(x_map):  17353.494140625 \n",
      "tau_alpha*np.sqrt(N):  2445.577521189034 \n",
      "N:  65536\n",
      "tau_alpha:  9.553037192144664\n",
      "gamma_alpha:  87575.00720868903\n"
     ]
    }
   ],
   "source": [
    "for img_name in img_name_list:\n",
    "    optim_iters = []\n",
    "    lci_uq_iters_arr = []\n",
    "\n",
    "    # %%\n",
    "    # Load image and mask\n",
    "    img, mat_mask = luq.helpers.load_imgs(img_name, repo_dir)\n",
    "\n",
    "    # Aliases\n",
    "    x = img\n",
    "    ground_truth = img\n",
    "\n",
    "    torch_img = torch.tensor(np.copy(img), dtype=myType, device=device).reshape(\n",
    "        (1, 1) + img.shape\n",
    "    )\n",
    "\n",
    "    phi = luq.operators.MaskedFourier_torch(\n",
    "        shape=img.shape, ratio=0.5, mask=mat_mask, norm=\"ortho\", device=device\n",
    "    )\n",
    "\n",
    "    y = phi.dir_op(torch_img).detach().cpu().squeeze().numpy()\n",
    "\n",
    "    # Define X Cai noise level\n",
    "    eff_sigma = luq.helpers.compute_complex_sigma_noise(y, input_snr)\n",
    "    sigma = eff_sigma * np.sqrt(2)\n",
    "\n",
    "    # Generate noise\n",
    "    rng = np.random.default_rng(seed=0)\n",
    "    n_re = rng.normal(0, eff_sigma, y[y != 0].shape)\n",
    "    n_im = rng.normal(0, eff_sigma, y[y != 0].shape)\n",
    "    # Add noise\n",
    "    y[y != 0] += n_re + 1.0j * n_im\n",
    "\n",
    "    # Observation\n",
    "    torch_y = torch.tensor(np.copy(y), device=device, dtype=myComplexType).reshape(\n",
    "        (1,) + img.shape\n",
    "    )\n",
    "    x_init = torch.abs(phi.adj_op(torch_y))\n",
    "\n",
    "    # %%\n",
    "    # Define the likelihood\n",
    "    likelihood = luq.operators.L2Norm_torch(\n",
    "        sigma=sigma,\n",
    "        data=torch_y,\n",
    "        Phi=phi,\n",
    "    )\n",
    "    # Lipschitz constant computed automatically by likelihood, stored in likelihood.beta\n",
    "\n",
    "    # Define real prox\n",
    "    cvx_set_prox_op = luq.operators.RealProx_torch()\n",
    "\n",
    "    # %%\n",
    "    # Load CRR model\n",
    "    torch.set_grad_enabled(False)\n",
    "    torch.set_num_threads(4)\n",
    "\n",
    "    exp_name = f\"Sigma_{sigma_training}_t_{t_model}/\"\n",
    "    model = utils_cvx_reg.load_model(\n",
    "        CRR_dir_name + exp_name, \"cuda:0\", device_type=\"gpu\"\n",
    "    )\n",
    "\n",
    "    print(f\"Numbers of parameters before prunning: {model.num_params}\")\n",
    "    model.prune()\n",
    "    print(f\"Numbers of parameters after prunning: {model.num_params}\")\n",
    "\n",
    "    # L_CRR = model.L.detach().cpu().squeeze().numpy()\n",
    "    # print(f\"Lipschitz bound {L_CRR:.3f}\")\n",
    "\n",
    "    # [not required] intialize the eigen vector of dimension (size, size) associated to the largest eigen value\n",
    "    model.initializeEigen(size=100)\n",
    "    # compute bound via a power iteration which couples the activations and the convolutions\n",
    "    model.precise_lipschitz_bound(n_iter=100)\n",
    "    # the bound is stored in the model\n",
    "    L_CRR = model.L.data.item()\n",
    "    print(f\"Lipschitz bound {L_CRR:.3f}\")\n",
    "\n",
    "    # %\n",
    "    for it_1 in range(len(reg_params)):\n",
    "        # Prior parameters\n",
    "        lmbd = reg_params[it_1]\n",
    "\n",
    "        # Compute stepsize\n",
    "        alpha = 0.98 / (likelihood.beta + mu * lmbd * L_CRR)\n",
    "\n",
    "        # initialization\n",
    "        x_hat = torch.clone(x_init)\n",
    "        z = torch.clone(x_init)\n",
    "        t = 1\n",
    "\n",
    "        for it_2 in range(options[\"iter\"]):\n",
    "            x_hat_old = torch.clone(x_hat)\n",
    "            x_hat = z - alpha * (likelihood.grad(z) + lmbd * model(mu * z))\n",
    "            # Reality constraint\n",
    "            x_hat = cvx_set_prox_op.prox(x_hat)\n",
    "            # Positivity constraint\n",
    "            # x = torch.clamp(x, 0, None)\n",
    "\n",
    "            t_old = t\n",
    "            t = 0.5 * (1 + math.sqrt(1 + 4 * t**2))\n",
    "            z = x_hat + (t_old - 1) / t * (x_hat - x_hat_old)\n",
    "\n",
    "            # relative change of norm for terminating\n",
    "            res = (torch.norm(x_hat_old - x_hat) / torch.norm(x_hat_old)).item()\n",
    "\n",
    "            if res < options[\"tol\"]:\n",
    "                print(\"[GD] converged in %d iterations\" % (it_2))\n",
    "                break\n",
    "\n",
    "            if it_2 % options[\"update_iter\"] == 0:\n",
    "                print(\n",
    "                    \"[GD] %d out of %d iterations, tol = %f\"\n",
    "                    % (\n",
    "                        it_2,\n",
    "                        options[\"iter\"],\n",
    "                        res,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        optim_iters.append(it_2)\n",
    "\n",
    "        # %%\n",
    "        np_x_init = to_numpy(x_init)\n",
    "        np_x = np.copy(x)\n",
    "        np_x_hat = to_numpy(x_hat)\n",
    "\n",
    "        images = [np_x, np_x_init, np_x_hat, np_x - np.abs(np_x_hat)]\n",
    "\n",
    "        # %%\n",
    "        labels = [\"Truth\", \"Dirty\", \"Reconstruction\", \"Residual (x - x^hat)\"]\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 8), dpi=200)\n",
    "        for i in range(4):\n",
    "            im = axs[i].imshow(\n",
    "                images[i],\n",
    "                cmap=cmap,\n",
    "                vmax=np.nanmax(images[i]),\n",
    "                vmin=np.nanmin(images[i]),\n",
    "            )\n",
    "            divider = make_axes_locatable(axs[i])\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            if i == 0:\n",
    "                stats_str = \"\\nRegCost {:.3f}\".format(\n",
    "                    model.cost(to_tensor(mu * images[i], device=device))[0].item()\n",
    "                )\n",
    "            if i > 0:\n",
    "                stats_str = \"\\n(PSNR: {:.2f}, SNR: {:.2f},\\nSSIM: {:.2f}, RegCost: {:.3f})\".format(\n",
    "                    psnr(np_x, images[i], data_range=np_x.max() - np_x.min()),\n",
    "                    luq.utils.eval_snr(x, images[i]),\n",
    "                    ssim(np_x, images[i], data_range=np_x.max() - np_x.min()),\n",
    "                    model.cost(to_tensor(mu * images[i], device=device))[0].item(),\n",
    "                )\n",
    "            labels[i] += stats_str\n",
    "            axs[i].set_title(labels[i], fontsize=16)\n",
    "            axs[i].axis(\"off\")\n",
    "        if save_fig_vals:\n",
    "            plt.savefig(\n",
    "                \"{:s}{:s}_lmbd_{:.1e}_optim_MAP.pdf\".format(savefig_dir, img_name, lmbd)\n",
    "            )\n",
    "        plt.close()\n",
    "\n",
    "        ### MAP-based UQ\n",
    "\n",
    "        # function handles to used for ULA\n",
    "        def _fun(_x, model, mu, lmbd):\n",
    "            return (lmbd / mu) * model.cost(mu * _x) + likelihood.fun(_x)\n",
    "\n",
    "        def _grad_fun(_x, likelihood, model, mu, lmbd):\n",
    "            return torch.real(likelihood.grad(_x) + lmbd * model(mu * _x))\n",
    "\n",
    "        def _prior_fun(_x, model, mu, lmbd):\n",
    "            return (lmbd / mu) * model.cost(mu * _x)\n",
    "\n",
    "        # Evaluation of the potentials\n",
    "        fun = partial(_fun, model=model, mu=mu, lmbd=lmbd)\n",
    "        prior_fun = partial(_prior_fun, model=model, mu=mu, lmbd=lmbd)\n",
    "        # Evaluation of the gradient\n",
    "        grad_f = partial(\n",
    "            _grad_fun, likelihood=likelihood, model=model, mu=mu, lmbd=lmbd\n",
    "        )\n",
    "        # Evaluation of the potential in numpy\n",
    "        fun_np = lambda _x: fun(luq.utils.to_tensor(_x, dtype=myType)).item()\n",
    "\n",
    "        # Compute HPD region bound\n",
    "        N = np_x_hat.size\n",
    "        tau_alpha = np.sqrt(16 * np.log(3 / alpha_prob))\n",
    "        gamma_alpha = fun(x_hat).item() + tau_alpha * np.sqrt(N) + N\n",
    "\n",
    "        error_p_arr = []\n",
    "        error_m_arr = []\n",
    "        mean_img_arr = []\n",
    "        computing_time = []\n",
    "\n",
    "        x_init_np = luq.utils.to_numpy(x_init)\n",
    "\n",
    "        # Compute ground truth block\n",
    "        gt_mean_img_arr = []\n",
    "        for superpix_size in superpix_MAP_sizes:\n",
    "            mean_image = ski.measure.block_reduce(\n",
    "                np.copy(img), block_size=(superpix_size, superpix_size), func=np.mean\n",
    "            )\n",
    "            gt_mean_img_arr.append(mean_image)\n",
    "\n",
    "        # Define prefix\n",
    "        save_MAP_prefix = \"{:s}_CRR_UQ_MAP_lmbd_{:.1e}\".format(img_name, lmbd)\n",
    "\n",
    "        for it_pixs, superpix_size in enumerate(superpix_MAP_sizes):\n",
    "            pr_time_1 = time.process_time()\n",
    "            wall_time_1 = time.time()\n",
    "\n",
    "            (\n",
    "                error_p,\n",
    "                error_m,\n",
    "                mean,\n",
    "                lci_iters_cumul,\n",
    "            ) = luq.map_uncertainty.create_local_credible_interval(\n",
    "                x_sol=np_x_hat,\n",
    "                region_size=superpix_size,\n",
    "                function=fun_np,\n",
    "                bound=gamma_alpha,\n",
    "                iters=LCI_iters,\n",
    "                tol=LCI_tol,\n",
    "                bottom=LCI_bottom,\n",
    "                top=LCI_top,\n",
    "                return_iters=True,\n",
    "            )\n",
    "            pr_time_2 = time.process_time()\n",
    "            wall_time_2 = time.time()\n",
    "\n",
    "            # Save iteration number\n",
    "            lci_uq_iters_arr.append(lci_iters_cumul)\n",
    "\n",
    "            # Add values to array to save it later\n",
    "            error_p_arr.append(np.copy(error_p))\n",
    "            error_m_arr.append(np.copy(error_m))\n",
    "            mean_img_arr.append(np.copy(mean))\n",
    "            computing_time.append((pr_time_2 - pr_time_1, wall_time_2 - wall_time_1))\n",
    "            # Clip plot values\n",
    "            error_length = luq.utils.clip_matrix(\n",
    "                np.copy(error_p), clip_low_val, clip_high_val\n",
    "            ) - luq.utils.clip_matrix(np.copy(error_m), clip_low_val, clip_high_val)\n",
    "            # Recover the ground truth mean\n",
    "            gt_mean = gt_mean_img_arr[it_pixs]\n",
    "\n",
    "            vmin = np.min((gt_mean, mean, error_length))\n",
    "            vmax = np.max((gt_mean, mean, error_length))\n",
    "            # err_vmax= 0.6\n",
    "\n",
    "            # Plot UQ\n",
    "            fig = plt.figure(figsize=(24, 5))\n",
    "\n",
    "            plt.subplot(141)\n",
    "            ax = plt.gca()\n",
    "            ax.set_title(\"MAP estimation,\\n superpix = {:d}\".format(superpix_size))\n",
    "            im = ax.imshow(mean, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "            plt.subplot(142)\n",
    "            ax = plt.gca()\n",
    "            ax.set_title(\n",
    "                \"Residual (GT - MAP),\\n RMSE = {:.3e}\".format(\n",
    "                    np.sqrt(np.sum((gt_mean - mean) ** 2))\n",
    "                )\n",
    "            )\n",
    "            im = ax.imshow(gt_mean - mean, cmap=cmap)\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "            plt.subplot(143)\n",
    "            ax = plt.gca()\n",
    "            ax.set_title(\n",
    "                \"LCI (max={:.5f})\\n (<LCI>={:.5f})\".format(\n",
    "                    np.max(error_length), np.mean(error_length)\n",
    "                )\n",
    "            )\n",
    "            im = ax.imshow(error_length, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "\n",
    "            plt.subplot(144)\n",
    "            ax = plt.gca()\n",
    "            ax.set_title(\"LCI - min(LCI)\")\n",
    "            im = ax.imshow(\n",
    "                error_length - np.min(error_length), cmap=cmap, vmin=vmin, vmax=vmax\n",
    "            )\n",
    "            divider = make_axes_locatable(ax)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation=\"vertical\")\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xticks([])\n",
    "            if save_fig_vals:\n",
    "                plt.savefig(\n",
    "                    savefig_dir\n",
    "                    + save_MAP_prefix\n",
    "                    + \"_UQ-MAP_pixel_size_{:d}.pdf\".format(superpix_size)\n",
    "                )\n",
    "            plt.close()\n",
    "\n",
    "        print(\n",
    "            \"f(x_map): \",\n",
    "            likelihood.fun(x_hat).item(),\n",
    "            \"\\ng(x_map): \",\n",
    "            prior_fun(x_hat).item(),\n",
    "            \"\\ntau_alpha*np.sqrt(N): \",\n",
    "            tau_alpha * np.sqrt(N),\n",
    "            \"\\nN: \",\n",
    "            N,\n",
    "        )\n",
    "        print(\"tau_alpha: \", tau_alpha)\n",
    "        print(\"gamma_alpha: \", gamma_alpha.item())\n",
    "        #\n",
    "        opt_params = {\n",
    "            \"lmbd\": lmbd,\n",
    "            \"mu\": mu,\n",
    "            \"sigma_training\": sigma_training,\n",
    "            \"t_model\": t_model,\n",
    "            \"sigma_noise\": sigma,\n",
    "            \"eff_sigma_noise\": eff_sigma,\n",
    "            \"opt_tol\": options[\"tol\"],\n",
    "            \"opt_max_iter\": options[\"iter\"],\n",
    "        }\n",
    "        hpd_results = {\n",
    "            \"alpha\": alpha_prob,\n",
    "            \"gamma_alpha\": gamma_alpha,\n",
    "            \"f_xmap\": likelihood.fun(x_hat).item(),\n",
    "            \"g_xmap\": prior_fun(x_hat).item(),\n",
    "            \"h_alpha_N\": tau_alpha * np.sqrt(N) + N,\n",
    "        }\n",
    "        LCI_params = {\n",
    "            \"iters\": LCI_iters,\n",
    "            \"tol\": LCI_tol,\n",
    "            \"bottom\": LCI_bottom,\n",
    "            \"top\": LCI_top,\n",
    "            \"clip_low_val\": clip_low_val,\n",
    "            \"clip_high_val\": clip_high_val,\n",
    "        }\n",
    "        save_map_vars = {\n",
    "            \"x_ground_truth\": img,\n",
    "            \"x_map\": np_x_hat,\n",
    "            \"x_init\": np_x_init,\n",
    "            \"opt_params\": opt_params,\n",
    "            \"hpd_results\": hpd_results,\n",
    "            \"error_p_arr\": error_p_arr,\n",
    "            \"error_m_arr\": error_m_arr,\n",
    "            \"mean_img_arr\": mean_img_arr,\n",
    "            \"gt_mean_img_arr\": gt_mean_img_arr,\n",
    "            \"computing_time\": computing_time,\n",
    "            \"superpix_sizes\": superpix_MAP_sizes,\n",
    "            \"LCI_params\": LCI_params,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number for W28\n",
      "Optimisation iterations:  541\n",
      "LCI iterations 16x16 super pixe:  21188\n",
      "LCI iterations 8x8 super pixe:  81540\n"
     ]
    }
   ],
   "source": [
    "print(\"Iteration number for W28\")\n",
    "\n",
    "print(\"Optimisation iterations: \", optim_iters[0])\n",
    "print(\"LCI iterations 16x16 super pixe: \", lci_uq_iters_arr[0])\n",
    "print(\"LCI iterations 8x8 super pixe: \", lci_uq_iters_arr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convex_uq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bb75ebd6ceb1eff2ce987e124c91bc6f99e62fd1930d98a82dc138614104eef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
