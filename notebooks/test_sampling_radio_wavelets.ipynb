{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "M1 = True\n",
        "\n",
        "if M1:\n",
        "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "else:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(torch.cuda.is_available())\n",
        "    print(torch.cuda.device_count())\n",
        "    print(torch.cuda.current_device())\n",
        "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "from functools import partial\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import time as time\n",
        "\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torchmetrics.functional import structural_similarity_index_measure \n",
        "from torchmetrics.functional import peak_signal_noise_ratio \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import scipy.io as sio\n",
        "from astropy.io import fits\n",
        "\n",
        "import large_scale_UQ as luq\n",
        "from large_scale_UQ.utils import to_numpy, to_tensor\n",
        "from convex_reg import utils as utils_cvx_reg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Optimisation options for the MAP estimation\n",
        "options = {\"tol\": 1e-5, \"iter\": 5000, \"update_iter\": 50, \"record_iters\": False}\n",
        "# Save param\n",
        "repo_dir = '/disk/xray0/tl3/repos/large-scale-UQ'\n",
        "repo_dir = '/Users/tl/Documents/research/repos/proj-convex-UQ/large-scale-UQ'\n",
        "save_dir = repo_dir + '/debug/sampling-outputs/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_name = 'M31'\n",
        "\n",
        "# Load img\n",
        "img_path = repo_dir + '/data/imgs/{:s}.fits'.format(img_name)\n",
        "img_data = fits.open(img_path, memmap=False)\n",
        "\n",
        "# Loading the image and cast it to float\n",
        "img = np.copy(img_data[0].data)[0,:,:].astype(np.float64)\n",
        "# Flipping data\n",
        "img = np.flipud(img)\n",
        "\n",
        "# Aliases\n",
        "x = img\n",
        "ground_truth = img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Load op from X Cai\n",
        "mask_path = repo_dir + '/data/operators_masks/fourier_mask.mat'\n",
        "op_mask = sio.loadmat(mask_path)['Ma']\n",
        "\n",
        "# Matlab's reshape works with 'F'-like ordering\n",
        "mat_mask = np.reshape(np.sum(op_mask, axis=0), (256,256), order='F').astype(bool)\n",
        "\n",
        "# %%\n",
        "\n",
        "torch_img = torch.tensor(np.copy(img), dtype=torch.float, device=device).reshape((1,1) + img.shape)\n",
        "\n",
        "# %%\n",
        "dim = 256\n",
        "phi = luq.operators.MaskedFourier_torch(\n",
        "    dim=dim, \n",
        "    ratio=0.5 ,\n",
        "    mask=mat_mask,\n",
        "    norm='ortho',\n",
        "    device=device\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Trying to convert ComplexFloat to the MPS backend but it does not have support for that dtype.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Define X Cai noise level\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sigma \u001b[39m=\u001b[39m \u001b[39m0.0024\u001b[39m\n\u001b[0;32m----> 4\u001b[0m y \u001b[39m=\u001b[39m phi\u001b[39m.\u001b[39;49mdir_op(torch_img)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      6\u001b[0m \u001b[39m# Generate noise\u001b[39;00m\n\u001b[1;32m      7\u001b[0m rng \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mdefault_rng(seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/research/repos/proj-convex-UQ/large-scale-UQ/large_scale_UQ/operators.py:339\u001b[0m, in \u001b[0;36mMaskedFourier_torch._torch_dir_op\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_torch_dir_op\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    326\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Computes the forward operator of the class.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m    Compute FFT and then mask Fourier coefficients.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39m        torch.Tensor: tensor of Fourier coefficients. Same shape as input.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmul(\n\u001b[0;32m--> 339\u001b[0m         torch\u001b[39m.\u001b[39;49mfft\u001b[39m.\u001b[39;49mfft2(x, norm\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm),\n\u001b[1;32m    340\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask\n\u001b[1;32m    341\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: Trying to convert ComplexFloat to the MPS backend but it does not have support for that dtype."
          ]
        }
      ],
      "source": [
        "# Define X Cai noise level\n",
        "sigma = 0.0024\n",
        "\n",
        "y = phi.dir_op(torch_img).detach().cpu().squeeze().numpy()\n",
        "\n",
        "# Generate noise\n",
        "rng = np.random.default_rng(seed=0)\n",
        "n = rng.normal(0, sigma, y[y!=0].shape)\n",
        "# Add noise\n",
        "y[y!=0] += n\n",
        "\n",
        "# Observation\n",
        "torch_y = torch.tensor(np.copy(y), device=device).reshape((1,1) + img.shape)\n",
        "x_init = torch.abs(phi.adj_op(torch_y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = luq.operators.l2_norm_torch(\n",
        "    sigma=sigma,\n",
        "    data=torch_y,\n",
        "    Phi=phi,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "convex_uq",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "2bb75ebd6ceb1eff2ce987e124c91bc6f99e62fd1930d98a82dc138614104eef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
