{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n",
            "0\n",
            "NVIDIA A100-PCIE-40GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "M1 = False\n",
        "\n",
        "if M1:\n",
        "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "else:\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if torch.cuda.is_available():\n",
        "        print(torch.cuda.is_available())\n",
        "        print(torch.cuda.device_count())\n",
        "        print(torch.cuda.current_device())\n",
        "        print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "import math\n",
        "import time as time\n",
        "\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from torchmetrics.functional import structural_similarity_index_measure \n",
        "from torchmetrics.functional import peak_signal_noise_ratio \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(font_scale=1.5)\n",
        "# plt.style.use('dark_background')\n",
        "plt.rcParams[\"font.family\"] = \"serif\"\n",
        "\n",
        "import scipy.io as sio\n",
        "from astropy.io import fits\n",
        "\n",
        "import large_scale_UQ as luq\n",
        "from large_scale_UQ.utils import to_numpy, to_tensor\n",
        "from convex_reg import utils as utils_cvx_reg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimisation options for the MAP estimation\n",
        "options = {\"tol\": 1e-5, \"iter\": 5000, \"update_iter\": 4999, \"record_iters\": False}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Save param\n",
        "repo_dir = '/disk/xray0/tl3/repos/large-scale-UQ'\n",
        "# repo_dir = '/Users/tl/Documents/research/repos/proj-convex-UQ/large-scale-UQ'\n",
        "save_dir = repo_dir + '/debug/torch_output_reg_strength/outputs/'\n",
        "savefig_dir = repo_dir + '/debug/torch_output_reg_strength/figs/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: The following header keyword is invalid or follows an unrecognized non-standard convention:\n",
            "INSTRUME                                                                         [astropy.io.fits.card]\n"
          ]
        }
      ],
      "source": [
        "img_name = 'M31'\n",
        "\n",
        "# Load img\n",
        "img_path = repo_dir + '/data/imgs/{:s}.fits'.format(img_name)\n",
        "img_data = fits.open(img_path, memmap=False)\n",
        "\n",
        "# Loading the image and cast it to float\n",
        "img = np.copy(img_data[0].data)[0,:,:].astype(np.float64)\n",
        "# Flipping data\n",
        "img = np.flipud(img)\n",
        "\n",
        "# Aliases\n",
        "x = img\n",
        "ground_truth = img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%\n",
        "# Load op from X Cai\n",
        "mask_path = repo_dir + '/data/operators_masks/fourier_mask.mat'\n",
        "op_mask = sio.loadmat(mask_path)['Ma']\n",
        "\n",
        "# Matlab's reshape works with 'F'-like ordering\n",
        "mat_mask = np.reshape(np.sum(op_mask, axis=0), (256,256), order='F').astype(bool)\n",
        "\n",
        "# %%\n",
        "\n",
        "torch_img = torch.tensor(np.copy(img), dtype=torch.float32, device=device).reshape((1,1) + img.shape)\n",
        "\n",
        "# %%\n",
        "dim = 256\n",
        "phi = luq.operators.MaskedFourier_torch(\n",
        "    dim=dim, \n",
        "    ratio=0.5 ,\n",
        "    mask=mat_mask,\n",
        "    norm='ortho',\n",
        "    device=device\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define X Cai noise level\n",
        "sigma = 0.0024\n",
        "\n",
        "y = phi.dir_op(torch_img).detach().cpu().squeeze().numpy()\n",
        "\n",
        "# Generate noise\n",
        "rng = np.random.default_rng(seed=0)\n",
        "n = rng.normal(0, sigma, y[y!=0].shape)\n",
        "# Add noise\n",
        "y[y!=0] += n\n",
        "\n",
        "# Observation\n",
        "torch_y = torch.tensor(np.copy(y), device=device, dtype=torch.complex64).reshape((1,) + img.shape)\n",
        "x_init = torch.abs(phi.adj_op(torch_y))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the likelihood\n",
        "g = luq.operators.L2Norm_torch(\n",
        "    sigma=sigma,\n",
        "    data=torch_y,\n",
        "    Phi=phi,\n",
        ")\n",
        "g.beta = 1.0 / sigma ** 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- loading checkpoint from epoch 10 ---\n",
            "---------------------\n",
            "Building a CRR-NN model with \n",
            " - [1, 8, 32] channels \n",
            " - linear_spline activation functions\n",
            "  (LinearSpline(mode=conv, num_activations=32, init=zero, size=21, grid=0.010, monotonic_constraint=True.))\n",
            "---------------------\n",
            "Numbers of parameters before prunning: 13610\n",
            "---------------------\n",
            " PRUNNING \n",
            " Found 22 filters with non-vanishing potential functions\n",
            "---------------------\n",
            "Numbers of parameters after prunning: 4183\n",
            "Lipschitz bound 0.770\n",
            "Lipschitz bound 0.770\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda:0'\n",
        "torch.set_grad_enabled(False)\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "sigma_training = 5\n",
        "t_model = 5\n",
        "dir_name = '/disk/xray0/tl3/repos/convex_ridge_regularizers/trained_models/'\n",
        "exp_name = f'Sigma_{sigma_training}_t_{t_model}/'\n",
        "model = utils_cvx_reg.load_model(dir_name+exp_name, device, device_type='gpu')\n",
        "\n",
        "print(f'Numbers of parameters before prunning: {model.num_params}')\n",
        "model.prune()\n",
        "print(f'Numbers of parameters after prunning: {model.num_params}')\n",
        "\n",
        "L = model.L.detach().cpu().squeeze().numpy()\n",
        "print(f\"Lipschitz bound {L:.3f}\")\n",
        "\n",
        "# [not required] intialize the eigen vector of dimension (size, size) associated to the largest eigen value\n",
        "model.initializeEigen(size=100)\n",
        "# compute bound via a power iteration which couples the activations and the convolutions\n",
        "model.precise_lipschitz_bound(n_iter=100)\n",
        "# the bound is stored in the model\n",
        "L = model.L.data.item()\n",
        "print(f\"Lipschitz bound {L:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1.e+02, 1.e+06])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lmbd_list = np.logspace(start=np.log10(1e2), stop=np.log10(1e6), num=2)\n",
        "lmbd_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[GD] 0 out of 5000 iterations, tol = 0.407644\n",
            "[GD] 4999 out of 5000 iterations, tol = 0.000018\n",
            "-----------------------\n",
            "Updating spline coefficients for the reg cost\n",
            " (the gradient-step model is trained and intergration is required to compute the regularization cost)\n",
            "-----------------------\n",
            "Calculating credible interval for superpxiel:  (256, 256)\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "Calculating credible interval for superpxiel:  (256, 256)\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[GD] 0 out of 5000 iterations, tol = 0.005333\n",
            "[GD] converged in 1178 iterations\n",
            "Calculating credible interval for superpxiel:  (256, 256)\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "Calculating credible interval for superpxiel:  (256, 256)\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n",
            "[Bisection Method] There is no root in this range.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "x_hat_list = []\n",
        "x_hat_np_list = []\n",
        "gamma_alpha_list = []\n",
        "prior_list = []\n",
        "likelihood_list = []\n",
        "const_gamma_alpha_list = []\n",
        "psnr_map_list = []\n",
        "\n",
        "error_p_list = []\n",
        "error_m_list = []\n",
        "\n",
        "# LCI parameters\n",
        "superpix_sizes = [32, 16]#, 8]\n",
        "LCI_iters = 200\n",
        "LCI_tol = 1e-5\n",
        "LCI_bottom = -10\n",
        "LCI_top = 10\n",
        "\n",
        "alpha = 0.05\n",
        "\n",
        "prefix = 'UQ_MAP_CRR_reg_strength'\n",
        "\n",
        "mean_LCI = np.zeros((len(superpix_sizes), len(lmbd_list)))\n",
        "computing_time = np.zeros((len(superpix_sizes), len(lmbd_list)))\n",
        "\n",
        "for it_lmbd in range(len(lmbd_list)):\n",
        "\n",
        "    # Prior parameters\n",
        "    lmbd = lmbd_list[it_lmbd]\n",
        "    mu = 20\n",
        "\n",
        "    # Compute stepsize\n",
        "    alpha = 1. / ( 1. + g.beta + mu * lmbd * L)\n",
        "\n",
        "    # initialization\n",
        "    x_hat = torch.clone(x_init)\n",
        "    z = torch.clone(x_init)\n",
        "    t = 1\n",
        "\n",
        "\n",
        "    for it in range(options['iter']):\n",
        "        x_hat_old = torch.clone(x_hat)\n",
        "        # grad = g.grad(z.squeeze()) +  lmbd * model(mu * z)\n",
        "        x_hat = z - alpha *(\n",
        "            g.grad(z) + lmbd * model(mu * z)\n",
        "        )\n",
        "        # Reality constraint\n",
        "        x_hat =  torch.real(x_hat)\n",
        "\n",
        "        t_old = t \n",
        "        t = 0.5 * (1 + math.sqrt(1 + 4*t**2))\n",
        "        z = x_hat + (t_old - 1)/t * (x_hat - x_hat_old)\n",
        "\n",
        "        # relative change of norm for terminating\n",
        "        res = (torch.norm(x_hat_old - x_hat)/torch.norm(x_hat_old)).item()\n",
        "\n",
        "        if res < options['tol']:\n",
        "            print(\"[GD] converged in %d iterations\"%(it))\n",
        "            break\n",
        "\n",
        "        if it % options['update_iter'] == 0:\n",
        "            print(\n",
        "                \"[GD] %d out of %d iterations, tol = %f\" %(            \n",
        "                    it,\n",
        "                    options['iter'],\n",
        "                    res,\n",
        "                )\n",
        "            )\n",
        "\n",
        "    x_hat_list.append(x_hat)\n",
        "    x_hat_np_list.append(luq.utils.to_numpy(x_hat))\n",
        "\n",
        "    psnr_map = psnr(ground_truth, x_hat_np_list[it_lmbd], data_range=ground_truth.max()-ground_truth.min())\n",
        "    psnr_map_list.append(psnr_map)\n",
        "\n",
        "    N = x_hat_np_list[it_lmbd].size\n",
        "    tau_alpha = np.sqrt(16*np.log(3/alpha))\n",
        "\n",
        "    # _reg_fun = lambda h, _x: h.fun(h.dir_op(_x))\n",
        "    # def _reg_fun(_x, h):\n",
        "    #     return h.fun(h.dir_op(_x))\n",
        "    # reg_fun = partial(_reg_fun, h=h)\n",
        "\n",
        "    reg_fun = lambda _x : (lmbd / mu) * model.cost(mu * _x)\n",
        "\n",
        "    def _fun(_x, model, mu, lmbd):\n",
        "        return (lmbd / mu) * model.cost(mu * _x) + g.fun(_x)\n",
        "\n",
        "    fun = partial(_fun, model=model, mu=mu, lmbd=lmbd)\n",
        "\n",
        "    loss_fun_torch = lambda _x : fun(_x)\n",
        "    loss_fun_np = lambda _x : fun(luq.utils.to_tensor(_x, dtype=torch.float32)).item()\n",
        "    # loss_fun = lambda _x : g.fun(_x)\n",
        "\n",
        "    gamma_alpha_list.append(loss_fun_torch(x_hat).item() + tau_alpha*np.sqrt(N) + N)\n",
        "    prior_list.append(reg_fun(x_hat).item())\n",
        "    likelihood_list.append(g.fun(x_hat).item())\n",
        "    const_gamma_alpha_list.append(tau_alpha*np.sqrt(N) + N)\n",
        "\n",
        "\n",
        "\n",
        "    # Compute the LCI\n",
        "    error_p_arr = []\n",
        "    error_m_arr = []\n",
        "\n",
        "    x_init_np = luq.utils.to_numpy(x_init)\n",
        "\n",
        "    for it_pix, superpix_size in enumerate(superpix_sizes):\n",
        "\n",
        "        pr_time_1 = time.process_time()\n",
        "        wall_time_1 = time.time()\n",
        "\n",
        "        error_p, error_m, mean = luq.map_uncertainty.create_local_credible_interval(\n",
        "        x_sol=x_hat_np_list[it_lmbd],\n",
        "        region_size=superpix_size,\n",
        "        function=loss_fun_np,\n",
        "        bound=gamma_alpha_list[it_lmbd],\n",
        "        iters=LCI_iters,\n",
        "        tol=LCI_tol,\n",
        "        bottom=LCI_bottom,\n",
        "        top=LCI_top,\n",
        "        )\n",
        "        error_length = error_p - error_m\n",
        "\n",
        "        pr_time_2 = time.process_time()\n",
        "        wall_time_2 = time.time()\n",
        "\n",
        "        error_p_arr.append(np.copy(error_p))\n",
        "        error_m_arr.append(np.copy(error_m))\n",
        "        computing_time[it_pix, it_lmbd] = wall_time_2 - wall_time_1\n",
        "\n",
        "        mean_LCI[it_pix, it_lmbd] = np.mean(error_length)\n",
        "    \n",
        "    error_p_list.append(error_p_arr)\n",
        "    error_m_list.append(error_m_arr)\n",
        "\n",
        "params = {\n",
        "    'lmbd_list': lmbd_list,\n",
        "    'mu': mu,\n",
        "    'superpix_sizes': np.array(superpix_size),\n",
        "    'LCI_iters': LCI_iters,\n",
        "    'LCI_tol': LCI_tol,\n",
        "    'LCI_bottom': LCI_bottom,\n",
        "    'LCI_top': LCI_top,\n",
        "    'alpha': alpha,\n",
        "    'otpim_options': options,\n",
        "    'sigma_training': sigma_training,\n",
        "    't_model': t_model,\n",
        "    'sigma_noise': sigma,\n",
        "}\n",
        "\n",
        "save_vars = {\n",
        "    'X_MAPs': np.array(x_hat_np_list),\n",
        "    'mean_LCI': mean_LCI,\n",
        "    'computing_LCI_time': computing_time,\n",
        "    'gamma_alpha_list': np.array(gamma_alpha_list),\n",
        "    'prior_list': np.array(prior_list),\n",
        "    'likelihood_list': np.array(likelihood_list),\n",
        "    'const_gamma_alpha_list': np.array(const_gamma_alpha_list),\n",
        "    'error_p_list': np.array(error_p_list, dtype=object),\n",
        "    'error_m_list': np.array(error_m_list, dtype=object),\n",
        "    'params': params,\n",
        "}\n",
        "\n",
        "\n",
        "save_path = '{:s}{:s}{:s}'.format(save_dir, prefix, '_vars.npy')\n",
        "np.save(save_path, save_vars, allow_pickle=True)\n",
        "\n",
        "zoom_id = 12\n",
        "\n",
        "kwargs = dict(linewidth=2, linestyle='dashed', markersize=6, marker='^', alpha=0.5)\n",
        "def_cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "plt.figure(figsize=(10,12))\n",
        "axs = plt.gca()\n",
        "axs.plot(lmbd_list, np.array(likelihood_list), '-o', label='f(x_map)')\n",
        "axs.plot(lmbd_list, np.array(prior_list), '-o', label='g(x_map)')\n",
        "axs.plot(lmbd_list, np.array(const_gamma_alpha_list), '-o', label='tau_alpha')\n",
        "axs.legend(fontsize=18, loc='upper center')\n",
        "axs.set_ylabel(r'Potentials')\n",
        "axs.set_xlabel(r'Reg strength')\n",
        "ax2 = axs.twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list), color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "plt.savefig('{:s}{:s}{:s}'.format(savefig_dir, prefix, '_potentials_plot.pdf'))\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(10,12))\n",
        "axs = plt.gca()\n",
        "axs.plot(lmbd_list[:12], np.array(likelihood_list)[:zoom_id], '-o', label='f(x_map)')\n",
        "axs.plot(lmbd_list[:12], np.array(prior_list)[:zoom_id], '-o', label='g(x_map)')\n",
        "axs.plot(lmbd_list[:12], np.array(const_gamma_alpha_list)[:zoom_id], '-o', label='tau_alpha')\n",
        "axs.legend(fontsize=18, loc='upper center')\n",
        "axs.set_ylabel(r'Potentials')\n",
        "axs.set_xlabel(r'Reg strength')\n",
        "ax2 = axs.twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list)[:zoom_id], color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "plt.savefig('{:s}{:s}{:s}'.format(savefig_dir, prefix, '_potentials_plot_zoom.pdf'))\n",
        "plt.close()\n",
        "\n",
        "\n",
        "kwargs = dict(linewidth=2, linestyle='dashed', markersize=6, marker='^', alpha=0.5)\n",
        "def_cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "plt.figure(figsize=(10,12))\n",
        "axs = plt.gca()\n",
        "for it in range(len(superpix_sizes)):\n",
        "    axs.plot(lmbd_list, mean_LCI[it,:], '-o', label='Pix size %d'%(superpix_sizes[it]))\n",
        "axs.legend(fontsize=18, loc='upper center')\n",
        "axs.set_ylabel(r'Potentials')\n",
        "axs.set_xlabel(r'Reg strength')\n",
        "ax2 = axs.twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list), color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "plt.savefig('{:s}{:s}{:s}'.format(savefig_dir, prefix, '_LCI_mean_plot.pdf'))\n",
        "plt.close()\n",
        "# plt.show()\n",
        "\n",
        "kwargs = dict(linewidth=2, linestyle='dashed', markersize=6, marker='^', alpha=0.5)\n",
        "def_cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "plt.figure(figsize=(10,12))\n",
        "axs = plt.gca()\n",
        "for it in range(len(superpix_sizes)):\n",
        "    axs.plot(lmbd_list, mean_LCI[it,:zoom_id], '-o', label='Pix size %d'%(superpix_sizes[it]))\n",
        "axs.legend(fontsize=18, loc='upper center')\n",
        "axs.set_ylabel(r'Potentials')\n",
        "axs.set_xlabel(r'Reg strength')\n",
        "ax2 = axs.twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list)[:zoom_id], color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "plt.savefig('{:s}{:s}{:s}'.format(savefig_dir, prefix, '_LCI_mean_plot_zoom.pdf'))\n",
        "plt.close()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "kwargs = dict(linewidth=2, linestyle='dashed', markersize=6, marker='^', alpha=0.5)\n",
        "def_cmap = plt.get_cmap(\"tab10\")\n",
        "\n",
        "fig, axs = plt.subplots(2,2, figsize=(24,16))\n",
        "\n",
        "for it in range(len(superpix_sizes)):\n",
        "    axs[0,0].plot(lmbd_list, mean_LCI[it,:], '-o', label='Pix size %d'%(superpix_sizes[it]))\n",
        "axs[0,0].legend(fontsize=18)\n",
        "axs[0,0].set_ylabel(r'<LCI>')\n",
        "axs[0,0].set_xlabel(r'Reg strength')\n",
        "ax2 = axs[0,0].twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list), color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "\n",
        "# \n",
        "axs[1,0].plot(lmbd_list, np.array(likelihood_list), '-o', label='likelihood(x_map)')\n",
        "axs[1,0].plot(lmbd_list, np.array(prior_list), '-o', label='prior(x_map)')\n",
        "axs[1,0].plot(lmbd_list, np.array(const_gamma_alpha_list), '-o', label='tau_alpha')\n",
        "axs[1,0].legend(fontsize=18, loc='upper center')\n",
        "axs[1,0].set_ylabel(r'Potentials')\n",
        "axs[1,0].set_xlabel(r'Reg strength')\n",
        "ax2 = axs[1,0].twinx()\n",
        "ax2.plot(lmbd_list, np.array(psnr_map_list), color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "\n",
        "for it in range(len(superpix_sizes)):\n",
        "    axs[0,1].plot(lmbd_list, mean_LCI[it,:zoom_id], '-o', label='Pix size %d'%(superpix_sizes[it]))\n",
        "axs[0,1].legend(fontsize=18)\n",
        "axs[0,1].set_ylabel(r'<LCI>')\n",
        "axs[0,1].set_xlabel(r'Reg strength')\n",
        "ax2 = axs[0,1].twinx()\n",
        "ax2.plot(lmbd_list[:zoom_id], np.array(psnr_map_list)[:zoom_id], color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "# \n",
        "\n",
        "axs[1,1].plot(lmbd_list, np.array(likelihood_list)[:zoom_id], '-o', label='likelihood(x_map)')\n",
        "axs[1,1].plot(lmbd_list, np.array(prior_list)[:zoom_id], '-o', label='prior(x_map)')\n",
        "axs[1,1].plot(lmbd_list, np.array(const_gamma_alpha_list)[:zoom_id], '-o', label='tau_alpha')\n",
        "axs[1,1].legend(fontsize=18, loc='center right')\n",
        "axs[1,1].set_ylabel(r'Potentials')\n",
        "axs[1,1].set_xlabel(r'Reg strength')\n",
        "ax2 = axs[1,1].twinx()\n",
        "ax2.plot(lmbd_list[:zoom_id], np.array(psnr_map_list)[:zoom_id], color=def_cmap(3), **kwargs)\n",
        "ax2.set_ylabel(r'PSNR(x_MAP)')\n",
        "ax2.grid(False)\n",
        "plt.tight_layout()\n",
        "save_path = '{:s}{:s}{:s}'.format(savefig_dir, prefix, '_reg_strength_plot.pdf')\n",
        "plt.savefig(save_path)\n",
        "plt.close()\n",
        "# plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "aa = np.array(error_p_list, dtype=object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(8, 8)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "aa[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "convex_uq",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "2bb75ebd6ceb1eff2ce987e124c91bc6f99e62fd1930d98a82dc138614104eef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
